# üß† Universal Context Loading Rule

**CRITICAL: Read this at the START of EVERY conversation AND before EVERY response (all LLMs).**

## üîÑ Two-Stage Context Loading

### **Stage 1: Conversation Start (Full Context Load)**

At the start of a NEW conversation, read these files in order:

### **Stage 2: Before Every Response (Incremental Refresh)**

Before EVERY response, check for new learning and updates.

---

## 1Ô∏è‚É£ Query LILL-MQ (QuadIndex) for Memory

**Priority: CRITICAL - Query QuadIndex FIRST for fast context loading**

**CURRENT ARCHITECTURE (November 2025 - Simplified):**

### **At Conversation Start:**

Query QuadIndex for relevant principles and context:

**Use LILL-MQ (Meta Quantizer/QuadIndex):**

- **VectorStore** - Semantic search for "How do I...?" questions
- **MetadataStore** - Exact filters for validated principles, decisions by date
- **GraphStore** - Relationship traversal for "What depends on...?" questions
- **ReasoningStore** - Deep thinking for "Should we...?" decisions

**Example queries:**

```typescript
// Get recent principles
const result = await quadIndex.search({
  text: "recent decisions and insights",
  limit: 10,
  filters: { minConfidence: 0.7 },
});

// Get validated principles
const validated = await quadIndex.search({
  text: "architecture patterns",
  filters: { status: "validated", minConfidence: 0.8 },
});

// Get relationships
const related = await quadIndex.search({
  text: "TypeScript strictness",
  includeRelationships: true,
});
```

**Why QuadIndex?**

- ‚ö° **Fast** - In-memory vector search with metadata filters
- üéØ **Accurate** - Semantic search finds relevant principles even with different wording
- üîó **Connected** - GraphStore shows relationships between principles
- üß† **Smart** - ReasoningStore provides alternatives and lessons learned

**Storage Location:** `.lill/snapshots/` (QuadIndex snapshots saved every 5 minutes)

### **Before Every Response:**

Query QuadIndex for relevant context based on user's question:

1. **Identify query type** - Search, filter, relationship, or reasoning?
2. **Query appropriate store** - VectorStore, MetadataStore, GraphStore, or ReasoningStore
3. **Use results in response** - Cite principle IDs (P42, P87, etc.)
4. **Learn from interaction** - LILL-Meta extracts new principles during conversation

**Why This Matters:** QuadIndex is continuously updated by the watcher. New principles are indexed in real-time as conversations are processed.

**Format:** QuadIndex stores principles in 4 stores (Vector, Metadata, Graph, Reasoning) with snapshots in `.lill/snapshots/`

## 2Ô∏è‚É£ Read Auto-Generated Documentation (.augment/ folder)

**Priority: HIGH - Read these for project structure understanding**

Read as needed:

- `.augment/project-overview.md` - Auto-scanned project structure and package discovery

**Format:** Standard markdown, auto-generated by AI hooks.

**Important:** These files are automatically updated. Humans should not manually edit them.

## 3Ô∏è‚É£ Query Protected AI Documentation (.ai/ folder) - **Phase 8: RAG System** ‚úÖ

**Priority: HIGH - Use RAG queries for efficient context loading**

**CRITICAL:** These files are OFF-LIMITS for automatic systems. They are AI thinking space only.

### **NEW: Use GuidelinesRAG for Efficient Retrieval**

Instead of reading entire `.ai/` files (28,500 tokens), use **RAG queries** to retrieve only relevant sections (1,000 tokens):

**At Conversation Start:**

- Query: "What are the project guidelines and conventions?"
- Returns: Top 3-5 relevant sections from all `.ai/` files

**During Conversation (as needed):**

- Query: "How should I handle errors?" ‚Üí Returns relevant sections from code-style.md
- Query: "What's the testing philosophy?" ‚Üí Returns relevant sections from Testing-philosophy.md
- Query: "How do I publish packages?" ‚Üí Returns relevant sections from npm-publishing-checklist.md
- Query: "What are the design system rules?" ‚Üí Returns relevant sections from design-system.md

**Benefits:**

- ‚ö° **28.5x token reduction** (28,500 ‚Üí 1,000 tokens at conversation start)
- üéØ **Adaptive context** - Retrieves only what's relevant to current task
- üìà **Scales forever** - Works even as `.ai/` files grow to 90,000+ tokens

**Available Files (indexed by GuidelinesRAG):**

- `.ai/code-style.md` - Code style reference and guidelines
- `.ai/design-system.md` - Design system reference and UI/UX rules
- `.ai/npm-publishing-checklist.md` - Publishing checklist and release procedures
- `.ai/Testing-philosophy.md` - Testing philosophy and test strategy

**Format:** Standard markdown prose for AI thinking and reference.

**Important:** These files are for YOUR thinking, working, and decision-making. Automatic systems (MemoryFileWriter, CacheConsolidationAgent, etc.) must NEVER modify these files. See `.augment/rules/protected-ai-files.md` for enforcement rules.

**How to Use:**

1. **Don't read entire files** - Use RAG queries instead
2. **Query with natural language** - "How should I..." or "What's the rule for..."
3. **Get relevant chunks** - System returns top K matching sections
4. **Adaptive loading** - Only load what you need, when you need it

## üéØ Success Criteria

You have successfully loaded context when you can answer:

- ‚úÖ What is this project about? (AETHER - Distributed AI Mind with memory consolidation)
- ‚úÖ What phase are we in? (Phase 6 - Complete Pipeline)
- ‚úÖ What are the top 3 principles? (Query QuadIndex)
- ‚úÖ What was the last conversation about? (Query QuadIndex for recent context)
- ‚úÖ What's the current task? (Check conversation context)

If you cannot answer these questions, **STOP and read the files above.**

---

**This rule ensures continuous memory across all AI sessions in this experimental project!** üöÄ
